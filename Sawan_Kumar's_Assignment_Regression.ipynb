{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm3A7BxGXD2x"
      },
      "outputs": [],
      "source": [
        "# Name - SAWAN KUMAR\n",
        "# Email ID - sawankumar19062001@gmail.com\n",
        "# Contact Number - 8882022191\n",
        "# Course - Data Science with Generative AI\n",
        "# Assignment - Regression\n",
        "# Module - 3 (Milestone 3 (Machine Learning))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Theory Question\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. What is Simple Linear Regression?\n",
        "\n",
        "Simple Linear Regression is a statistical method to model the relationship between a single independent variable (X) and a dependent variable (Y) by fitting a linear equation of the form ( Y = mX + c ), where ( m ) is the slope and ( c ) is the intercept. It predicts the dependent variable based on the independent variable.\n",
        "#2. What are the key assumptions of Simple Linear Regression?\n",
        "The key assumptions are:\n",
        "\n",
        "Linearity: The relationship between X and Y is linear.\n",
        "\n",
        "Independence: Observations are independent of each other.\n",
        "\n",
        "Homoscedasticity: Constant variance of residuals across all levels of X.\n",
        "\n",
        "Normality: Residuals are normally distributed.\n",
        "\n",
        "No multicollinearity: Not applicable in simple linear regression (single predictor).\n",
        "\n",
        "#3. What does the coefficient ( m ) represent in the equation ( Y = mX + c )?\n",
        "\n",
        "The coefficient ( m ) represents the slope of the regression line. It indicates the change in the dependent variable ( Y ) for a one-unit increase in the independent variable ( X ).\n",
        "#4. What does the intercept ( c ) represent in the equation ( Y = mX + c )?\n",
        "\n",
        "The intercept ( c ) represents the value of the dependent variable ( Y ) when the independent variable ( X ) is zero. It is the point where the regression line crosses the Y-axis.\n",
        "#5. How do we calculate the slope ( m ) in Simple Linear Regression?\n",
        "\n",
        "The slope ( m ) is calculated using the formula:[m = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2}]where ( \\bar{X} ) and ( \\bar{Y} ) are the means of X and Y, respectively.\n",
        "#6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "The least squares method minimizes the sum of squared differences between observed values of Y and the values predicted by the linear model. It ensures the best-fitting line by reducing prediction errors.\n",
        "#7. How is the coefficient of determination ( R^2 ) interpreted in Simple Linear Regression?\n",
        "\n",
        "The coefficient of determination ( R^2 ) represents the proportion of variance in the dependent variable explained by the independent variable. It ranges from 0 to 1, where a higher ( R^2 ) indicates a better fit (e.g., ( R^2 = 0.8 ) means 80% of the variance is explained).\n",
        "#8. What is Multiple Linear Regression?\n",
        "\n",
        "Multiple Linear Regression models the relationship between one dependent variable and multiple independent variables using the equation ( Y = b_0 + b_1X_1 + b_2X_2 + \\dots + b_nX_n ), where ( b_0 ) is the intercept and ( b_1, b_2, \\dots, b_n ) are coefficients.\n",
        "#9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "Simple Linear Regression uses one independent variable, while Multiple Linear Regression uses multiple independent variables to predict the dependent variable.\n",
        "#10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "The assumptions are:\n",
        "\n",
        "\n",
        "Linearity: Linear relationship between predictors and the dependent variable.\n",
        "\n",
        "Independence: Observations are independent.\n",
        "\n",
        "Homoscedasticity: Constant variance of residuals.\n",
        "\n",
        "Normality: Residuals are normally distributed.\n",
        "\n",
        "No multicollinearity: Independent variables are not highly correlated with each other.\n",
        "\n",
        "#11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Heteroscedasticity occurs when the variance of residuals is not constant across levels of the independent variables. It can lead to inefficient estimates, biased standard errors, and unreliable hypothesis tests, affecting the model’s accuracy.\n",
        "#12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "To address high multicollinearity:\n",
        "\n",
        "\n",
        "Remove highly correlated predictors.\n",
        "Use dimensionality reduction techniques like Principal Component Analysis (PCA).\n",
        "Apply regularization methods like Ridge Regression or Lasso Regression.\n",
        "\n",
        "#13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "Common techniques include:\n",
        "\n",
        "\n",
        "Dummy Encoding: Convert categorical variables into binary (0/1) variables.\n",
        "\n",
        "One-Hot Encoding: Create a binary column for each category.\n",
        "\n",
        "Label Encoding: Assign numerical values to categories (used cautiously, as it implies order).\n",
        "\n",
        "#14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Interaction terms capture the combined effect of two or more independent variables on the dependent variable, allowing the model to account for non-additive relationships (e.g., ( X_1 \\cdot X_2 )).\n",
        "#15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "In Simple Linear Regression, the intercept is the predicted value of Y when X is zero. In Multiple Linear Regression, it is the predicted value of Y when all independent variables are zero, which may not always be meaningful if zero is outside the range of the predictors.\n",
        "#16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "The slope represents the change in the dependent variable for a one-unit increase in an independent variable, holding others constant (in Multiple Linear Regression). It determines the direction and magnitude of predictions.\n",
        "#17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "The intercept provides a baseline value for the dependent variable when all independent variables are zero, offering context for the starting point of the relationship modeled by the regression line.\n",
        "#18. What are the limitations of using ( R^2 ) as a sole measure of model performance?\n",
        "\n",
        "Limitations of ( R^2 ):\n",
        "\n",
        "It does not indicate the model’s predictive accuracy or residual distribution.\n",
        "\n",
        "It increases with more predictors, even if they are irrelevant.\n",
        "\n",
        "It does not account for overfitting or model complexity.\n",
        "\n",
        "#19. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "A large standard error indicates high variability in the estimated coefficient, suggesting it is less precise. This could result from small sample sizes, multicollinearity, or high variability in the data.\n",
        "#20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "Heteroscedasticity is identified in residual plots if the spread of residuals increases or decreases systematically with fitted values (e.g., a funnel shape). Addressing it is important because it can bias standard errors, leading to unreliable p-values and confidence intervals.\n",
        "#21. What does it mean if a Multiple Linear Regression model has a high ( R^2 ) but low adjusted ( R^2 )?\n",
        "\n",
        "A high ( R^2 ) but low adjusted ( R^2 ) suggests the model includes unnecessary predictors that inflate ( R^2 ). Adjusted ( R^2 ) penalizes for model complexity, indicating poor explanatory power relative to the number of predictors.\n",
        "#22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Scaling variables ensures all predictors are on the same scale, preventing variables with larger ranges from dominating the model. It improves coefficient interpretability and model convergence in some algorithms.\n",
        "#23. What is polynomial regression?\n",
        "\n",
        "Polynomial Regression is a type of regression where the relationship between the independent variable and the dependent variable is modeled as an nth-degree polynomial, rather than a straight line.\n",
        "#24. How does polynomial regression differ from linear regression?\n",
        "\n",
        "Polynomial Regression uses polynomial terms (e.g., ( X^2, X^3 )) to capture non-linear relationships, while Linear Regression assumes a linear relationship between variables.\n",
        "#25. When is polynomial regression used?\n",
        "\n",
        "Polynomial Regression is used when the relationship between the independent and dependent variables is non-linear but can be approximated by a polynomial function.\n",
        "#26. What is the general equation for polynomial regression?\n",
        "\n",
        "The general equation is:[Y = b_0 + b_1X + b_2X^2 + \\dots + b_nX^n]where ( b_0 ) is the intercept, and ( b_1, b_2, \\dots, b_n ) are coefficients for the polynomial terms.\n",
        "#27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Yes, polynomial regression can include multiple variables by adding polynomial terms for each variable and their interactions (e.g., ( X_1^2, X_2^2, X_1X_2 )).\n",
        "#28. What are the limitations of polynomial regression?\n",
        "\n",
        "Limitations include:\n",
        "\n",
        "\n",
        "Risk of overfitting with high-degree polynomials.\n",
        "\n",
        "Increased model complexity.\n",
        "\n",
        "Poor extrapolation beyond the range of the data.\n",
        "\n",
        "Sensitivity to outliers.\n",
        "\n",
        "#29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "Methods include:\n",
        "\n",
        "\n",
        "Cross-Validation: Assess model performance on unseen data.\n",
        "\n",
        "Adjusted ( R^2 ): Balances model fit and complexity.\n",
        "\n",
        "Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC):\n",
        "Penalize complex models.\n",
        "\n",
        "Residual Plots: Check for patterns indicating poor fit.\n",
        "\n",
        "#30. Why is visualization important in polynomial regression?\n",
        "\n",
        "Visualization helps assess the fit of the polynomial model, identify non-linear patterns, detect overfitting, and ensure the model captures the data’s underlying trend effectively.\n",
        "#31. How is polynomial regression implemented in Python?\n",
        "\n",
        "Polynomial regression can be implemented using libraries like scikit-learn. Below is an example using PolynomialFeatures and LinearRegression:\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "scQMNO5Ab4MV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V_UlxJvXcyIy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}